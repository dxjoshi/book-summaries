- **Chapter-1: Microservices**
    - Microservices take this same approach to independent services. We focus our service boundaries on business boundaries, making it obvious where code lives for a given piece of functionality.(Single Responsibility Principle)    
    - **Service-oriented architecture (SOA)** is a design approach where multiple services collaborate to provide some end set of capabilities. A service here typically means a completely separate operating system process. Communication between these services occurs via calls across a network rather than method calls within a process boundary.

- **Chapter-2: The Evolutionary Architect**
    - Defining clear attributes that each service should have is one way of being clear as to where that balance sits.
        - **Monitoring:** It is essential that we are able to draw up coherent, cross-service views of our system health. This has to be a system-wide view, not a service-specific view.
        - **Interfaces:** Picking a small number of defined interface technologies helps integrate new consumers.       
        - **Architectural Safety:** We cannot afford for one badly behaved service to ruin the party for everyone. We have to ensure that our services shield themselves accordingly from unhealthy, downstream calls. The more services we have that do not properly handle the potential failure of downstream calls, the more fragile our systems will be.   
    - Getting together and agreeing on how things can be done is a good idea. I am a great believer in making it easy to do the right thing. Two techniques I have seen work well here are using **exemplars and providing service templates**. 
        - **Exemplars:** If you have a set of standards or best practices you would like to encourage, then having exemplars that you can point people to is useful.   
        - **Tailored Service Template:** What if, out of the box, the developers had most of the code in place to implement the core attributes that each service needs? Dropwizard and Karyon are two open source, JVM-based microcontainers. They work in similar ways, pulling together a set of libraries to provide features like health checking, serving HTTP, or exposing metrics. So, out of the box, you have a service complete with an embedded servlet container that can be launched from the command line.   
        - **Technical Debt:** When we accrue technical debt, just like debt in the real world it has an ongoing cost, and is something we want to pay down. 
        - **Exception Handling:** So our principles and practices guide how our systems should be built. But what happens when our system deviates from this? Sometimes we make a decision that is just an exception to the rule. In these cases, it might be worth capturing such a decision in a log somewhere for future reference. If enough exceptions are found, it may eventually make sense to change the principle or practice to reflect a new understanding of the world. For example, we might have a practice that states that we will always use MySQL for data storage. But then we see compelling reasons to use Cassandra for highly scalable storage, at which point we change our practice to say, “Use MySQL for most storage requirements, unless you expect large growth in volumes, in which case use Cassandra.”    
    - **Architects are responsible for a lot of things:** They need to ensure there is a set of principles that can guide development, and that these principles match the organization’s strategy. They need to make sure as well that these principles don’t require working practices that make developers miserable. They need to keep up to date with new technology, and know when to make the right trade-offs. This is an awful lot of responsibility. All that, and they also need to carry people with them—that is, to ensure that the colleagues they are working with understand the decisions being made and are brought in to carry them out. Oh, and as we’ve already mentioned: they need to spend some time with the teams to understand the impact of their decisions, and perhaps even code too.    
            
- **Chapter-3: How to Model Services**
    - **What Makes a Good Service?:**   
        - I want you to focus on two key concepts: loose coupling and high cohesion. We’ll talk in detail throughout the book about other ideas and practices, but they are all for naught if we get these two thing wrong.     
        - **Loose Coupling** When services are loosely coupled, a change to one service should not require a change to another.     
        - **High Cohesion** We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release that change as soon as possible. If we have to change that behavior in lots of different places, we’ll have to release lots of different services (perhaps at the same time) to deliver that change. Making changes in lots of different places is slower, and deploying lots of services at once is risky—both of which we want to avoid.   
    - **The Bounded Context:**  
        - The idea is that any given domain consists of multiple bounded contexts, and residing within each are things (Eric uses the word model a lot, which is probably better than things) that do not need to be communicated outside as well as things that are shared externally with other bounded contexts. Each bounded context has an explicit interface, where it decides what models to share with other contexts.    
        - For MusicCorp, we can then consider the finance department and the warehouse to be two separate bounded contexts. They both have an explicit interface to the outside world (in terms of inventory reports, pay slips, etc.), and they have details that only they need to know about (forklift trucks, calculators).
        - Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though—for example it needs to know about stock levels to keep the accounts up to date. The stock item then becomes a shared model between the two contexts. However, note that we don’t need to blindly expose everything about the stock item from the warehouse context. For example, although internally we keep a record on a stock item as to where it should live within the warehouse, that doesn’t need to be exposed in the shared model.
        - Sometimes we may encounter models with the same name that have very different meanings in different contexts too. For example, we might have the concept of a return, which represents a customer sending something back. Within the context of the customer, a return is all about printing a shipping label, dispatching a package, and waiting for a refund. For the warehouse, this could represent a package that is about to arrive, and a stock item that needs to be restocked.
    - **Premature Decomposition:** Prematurely decomposing a system into microservices can be costly, especially if you are new to the domain. In many ways, having an existing codebase you want to decompose into microservices is much easier than trying to go to microservices from the beginning.    
    - **Business Capabilities:** When you start to think about the bounded contexts that exist in your organization, you should be thinking not in terms of data that is shared, but about the capabilities those contexts provide the rest of the domain.  
    
- **Chapter-4:Integration** 
    - **Looking for the Ideal Integration Technology**
        - **Avoid Breaking Changes:** Every now and then, we may make a change that requires our consumers to also change.    
        - **Keep Your APIs Technology-Agnostic:** I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between microservices technology-agnostic. This means avoiding integration technology that dictates what technology stacks we can use to implement our microservices.    
        - **Make Your Service Simple for Consumers**
        - **Hide Internal Implementation Detail** We don’t want our consumers to be bound to our internal implementation. This leads to increased coupling. 
        - **Interfacing with Customers**   
    - **The Shared Database:** Remember when we talked about the core principles behind good microservices? Strong cohesion and loose coupling—with database integration, we lose both things. Database integration makes it easy for services to share data, but does nothing about sharing behavior.   
    - **Synchronous Versus Asynchronous:** 
        - These two different modes of communication can enable two different idiomatic styles of collaboration: request/response or event-based. 
        - With request/response, a client initiates a request and waits for the response. This model clearly aligns well to synchronous communication, but can work for asynchronous communication too. I might kick off an operation and register a callback, asking the server to let me know when my operation has completed.         
        - With an event-based collaboration, we invert things. Instead of a client initiating requests asking for things to be done, it instead says this thing happened and expects other parties to know what to do. Event-based systems by their nature are asynchronous. The smarts are more evenly distributed— that is, the business logic is not centralized into core brains, but instead pushed out more evenly to the various collaborators. Event-based collaboration is also highly decoupled. The client that emits an event doesn’t have any way of knowing who or what will react to it, which also means that you can add new subscribers to these events without the client ever needing to know.  
    - **Orchestration Versus Choreography:**                                                                                  
        - With orchestration, we rely on a central brain to guide and drive the
          process, much like the conductor in an orchestra. With choreography, we inform
          each part of the system of its job, and let it work out the details, like dancers all finding
          their way and reacting to others around them in a ballet.
        - **Orchestration:** Here, probably the simplest thing to do would be to have our customer service act as the central brain. On creation, it talks to the loyalty points bank, email service, and postal service, through a series of request/response calls. The customer service itself can then track where a customer is in this process. It can check to see if the customer’s account has been set up, or the email sent, or the post delivered.   
        - **With a choreographed approach:**, we could instead just have the customer service emit an event in an asynchronous manner, saying Customer created. The email service, postal service, and loyalty points bank then just subscribe to these events and react accordingly. This approach is significantly more decoupled. If some other service needed to reach to the creation of a customer, it just needs to subscribe to the events and do its job when needed.
    - **Remote procedure call** refers to the technique of making a local call and having it execute on a remote service somewhere.    
        - **Downsides to REST Over HTTP:**  
            - A more minor point is that some web server frameworks don’t actually support all the HTTP verbs well. 
            - REST over HTTP payloads can actually be more compact than SOAP because it supports alternative formats like JSON or even binary, but it will still be nowhere near as lean a binary protocol as Thrift might be. The overhead of HTTP for each request may also be a concern for low-latency requirements.    
    - **DRY and the Perils of Code Reuse in a Microservice World:**  
        - One of the acronyms we developers hear a lot is **DRY: don’t repeat yourself**. Though its definition is sometimes simplified as trying to avoid duplicating code, DRY more accurately means that we want to avoid duplicating our system behavior and knowledge. This is very sensible advice in general. Having lots of lines of code that do the same thing makes your codebase larger than needed, and therefore harder to reason about. When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs.   
    - **Versioning:**    
        - A client trying to be as flexible as possible in consuming a service demonstrates Postel’s Law (otherwise known as the robustness principle), which states: “Be conservative in what you do, be liberal in what you accept from others.”        
        - **Semantic versioning** is a specification that allows just that. With semantic versioning, each version number is in the form MAJOR.MINOR.PATCH. When the MAJOR number increments, it means that backward incompatible changes have been made. When MINOR increments, new functionality has been added that should be backward compatible. Finally, a change to PATCH states that bug fixes have been made to existing functionality.    

- **Chapter-5:Splitting the Monolith:** 
    - In his book Working Effectively with Legacy Code, Michael Feathers defines the concept of a seam—that is, a portion of the code that can be treated in isolation and worked on without impacting the rest of the codebase. We also want to identify seams. But rather than finding them for the purpose of cleaning up our codebase, we want to identify seams that can become service boundaries.    
    - **The Reasons to Split the Monolith:**    
        - Pace of Change
        - Team Structure
        - Security        
        - Technology
    - **Tangled Dependencies:**
        - The Database    
        
- **Chapter-6:Deployment**
    - With Continuous integration(CI), the core goal is to keep everyone in sync with each other, which we achieve by making sure that newly checked-in code properly integrates with existing code. To do this, a CI server detects that the code has been committed, checks it out, and carries out some verification like making sure the code compiles and that tests pass. 
    - **Mapping Continuous Integration to Microservices:**  The approach I prefer is to have a single CI build per microservice, to allow us to quickly make and validate a change prior to deployment into production. Here each microservice has its own source code repository, mapped to its own CI build. When making a change, I run only the build and tests I need to. I get a single artifact to deploy. Alignment to team ownership is more clear too. If you own the service, you own the repository and the build. Making changes across repositories can be more difficult in this world, but I’d maintain this is easier to resolve (e.g., by using command-line scripts) than the downside of the monolithic source control and build process.
    - **Build Pipelines:** Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this comes into play. I may have a lot of fast, small-scoped tests, and a small number of largescoped, slow tests. If we run all the tests together, we may not be able to get fast feedback when our fast tests fail if we’re waiting for our long-scoped slow tests to finally finish. And if the fast tests fail, there probably isn’t much sense in running the slower tests anyway! A solution to this problem is to have different stages in our build, creating what is known as a build pipeline. One stage for the faster tests, one for the slower tests.  
    - **Continuous delivery (CD):** builds on this concept, and then some. As outlined in Jez Humble and Dave Farley’s book of the same name, continuous delivery is the approach whereby we get constant feedback on the production readiness of each and every check-in, and furthermore treat each and every check-in as a release candidate. To fully embrace this concept, we need to model all the processes involved in getting our software from check-in to production, and know where any given version of the software is in terms of being cleared for release. In CD, we do this by extending the idea of the multistage build pipeline to model each and every stage our software has to go through, both manual and automated.
    - The “one microservice per build” approach is absolutely something you should aim for, but are there times when something else makes sense? When a team is starting out with a new project, especially a greenfield one where they are working with a blank sheet of paper, it is quite likely that there will be a large amount of churn in terms of working out where the service boundaries lie. This is a good reason, in fact, for keeping your initial services on the larger side until your understanding of the domain stabilizes.    
    - **Environments:**
        - On my developer laptop I want to quickly deploy the service, potentially against stubbed collaborators, to run tests or carry out some manual validation of behavior, whereas when I deploy into a production environment I may want to deploy multiple copies of my service in a loadbalanced fashion, perhaps split across one or more data centers for durability reasons.
        - As you move from your laptop to build server to UAT environment all the way to production, you’ll want to ensure that your environments are more and more production-like to catch any problems associated with these environmental differences sooner. This will be a constant balance. Sometimes the time and cost to reproduce production-like environments can be prohibitive, so you have to make compromises. Additionally, sometimes using a production-like environment can slow down feedback loops; waiting for 25 machines to install your software in AWS might be much slower than simply deploying your service into a local Vagrant instance, for example.
    - **Service Configuration:** A better approach is to create one single artifact, and manage configuration separately. This could be a properties file that exists for each environment, or different parameters passed in to an install process. Another popular option, especially when dealing with a larger number of microservices, is to use a dedicated system for providing configuration.
    - Having multiple services per host, as shown in, is attractive for a number of reasons. First, purely from a host management point of view, it is simpler. In a world where one team manages the infrastructure and another team manages the software, the infrastructure team’s workload is often a function of the number of hosts it has to manage. If more services are packed on to a single host, the host management workload doesn’t increase as the number of services increases. Second is cost. Even if you have access to a virtualization platform that allows you to provision and resize virtual hosts, the virtualization can add an overhead that reduces the underlying resources available to your services. 
    - This model is also familiar to those who deploy into some form of an application container. In some ways, the use of an application container is a special case of the multiple-services-per-host model, so we’ll look into that separately. This model can also simplify the life of the developer. Deploying multiple services to a single host in production is synonymous with deploying multiple services to a local dev workstation or laptop.            
    
- **Chapter-7:Testing**
    - **Types of Tests:**
        - At the bottom, we have tests that are technology-facing—that is, tests that aid the developers in creating the system in the first place. **Performance tests and smallscoped unit tests** fall into this category—all typically automated. 
        - This is compared with the top half of the quadrant, where tests help the nontechnical stakeholders understand how your system works. These could be **large-scoped, end-to-end tests, as shown in the top-left Acceptance Test square**, or manual testing as typified by user testing done against **a UAT system, as shown in the Exploratory Testing square**.
        - The trend recently has been away from any large-scale manual testing, in favor of automating as much as possible, and I certainly agree with this approach.    
        - If you have tests that sometimes fail, but everyone just re-runs them because they may pass again later, then you have **flaky tests**. When they fail, they don’t tell us much.
        - It is essential that we do our best to remove Flaky Tests. Otherwise, we start to lose faith in a test suite that “always fails like that.” This very human tendency means we need to find and eliminate these tests as soon as we can before we start to assume that failing tests are OK.   
        - Sometimes expending the same effort into getting better at remediation of a release can be significantly more beneficial than adding more automated functional tests. In the web operations world, this is often referred to as the trade-off between optimizing for **mean time between failures (MTBF) and mean time to repair (MTTR)**.    

- **Chapter-8:Monitoring**
    - We now have multiple servers to monitor, multiple logfiles to sift through, and multiple places where network latency could cause problems. So how do we approach this? The answer here is pretty straightforward: monitor the small things, and use aggregation to see the bigger picture.
    - If multiple services are running on multiple servers then the best option is collection and central aggregation of as much as we can get our hands on, from logs to application metrics.  
    
- **Chapter-9:Security**
    - In the context of security, authentication is the process by which we confirm that a party(AKA principal) is who she says she is.    
    - Authorization is the mechanism by which we map from a principal to the action we are allowing her to do.
    - A common approach to authentication and authorization is to use some sort of **single sign-on (SSO) solution**. SAML, which is the reigning implementation in the enterprise space, and OpenID Connect both provide capabilities in this area.    
    - When a principal tries to access a resource (like a web-based interface), she is directed to authenticate with an identity provider. This may ask her to provide a username and password, or might use something more advanced like two-factor authentication. Once the identity provider is satisfied that the principal has been authenticated, it gives information to the service provider, allowing it to decide whether to grant her access to the resource.    
    - This identity provider could be an externally hosted system, or something inside your own organization. Google, for example, provides an OpenID Connect identity provider. For enterprises, though, it is common to have your own identity provider, which may be linked to your company’s directory service. A directory service could be something like the Lightweight Directory Access Protocol (LDAP) or Active Directory. These systems allow you to store information about principals, such as what roles they play in the organization. Often, the directory service and the identity provider are one and the same, while sometimes they are separate but linked. Okta, for example, is a hosted SAML identity provider that handles tasks like two-factor authentication, but can link to your company’s directory services as the source of truth.    
    - **The deputy problem:** There is a type of vulnerability called the confused deputy problem, which in the context of service-to-service communication refers to a situation where a malicious party can trick a deputy service into making calls to a downstream service on his behalf that he shouldn’t be able to. For example, as a customer, when I log in to the online shopping system, I can see my account details. What if I could trick the online shopping UI into making a request for someone else’s details, maybe by making a call with my logged-in credentials?
    - **Securing Data at Rest:** For encryption at rest, unless you have a very good reason for picking something else, pick a well-known implementation of AES-128 or AES-256 for your platform.   

- **Chapter-10:Conway’s Law and System Design**
    